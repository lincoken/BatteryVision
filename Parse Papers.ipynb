{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dcm96\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dcm96\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re, string, unicodedata, nltk, pprint\n",
    "from nltk import word_tokenize\n",
    "import contractions\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import warnings\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadFile(filename=\"Sample.txt\"):\n",
    "    \"\"\"Reads in a text file and stores text as a string then closes the file.\"\"\"\n",
    "    file = open(filename,mode='r')\n",
    "    # read all lines at once\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(text): # Probably not needed\n",
    "    \"\"\"Strips off all html formatting\"\"\"\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def strip_until_start(paragraphs):\n",
    "    \"\"\"Takes array of text split by paragraphs and eliminates everything until\n",
    "        a certain keyword. If keyword isn't found then raises a warning.\"\"\"\n",
    "    for segment in paragraphs:\n",
    "        if \"Abstract\" in segment:\n",
    "            index = paragraphs.index(segment) + 1\n",
    "            return paragraphs[index:]\n",
    "        elif \"published\" in segment:\n",
    "            index = paragraphs.index(segment) + 1\n",
    "            return paragraphs[index:]\n",
    "#         else:\n",
    "#              warnings.warn(\"Couldn't find token to strip beginning of text.\")\n",
    "\n",
    "def strip_after_end(paragraphs):\n",
    "    \"\"\"Takes array of text split by paragraphs and eliminates everything after\n",
    "    a certain keyword. If keyword isn't found then raises a warning.\"\"\"\n",
    "    for segment in paragraphs:\n",
    "        if \"References\" in segment:\n",
    "            index = paragraphs.index(segment) \n",
    "            return paragraphs[:index]\n",
    "        elif \"references\" in segment:\n",
    "            index = paragraphs.index(segment) \n",
    "            return paragraphs[:index]\n",
    "#         else:\n",
    "#              warnings.warn(\"Couldn't find token to strip end of text.\")\n",
    "                \n",
    "def remove_spaces(paragraphs): # Run third\n",
    "    \"\"\"Removes empty spaces in paragraph segmented text.\"\"\"\n",
    "    for segment in paragraphs:\n",
    "        if segment == \"\":\n",
    "            paragraphs.remove(segment)\n",
    "        elif segment == \" \":            \n",
    "            paragraphs.remove(segment)\n",
    "    return paragraphs\n",
    "    \n",
    "def convert_to_array(paragraphs, cutoff=3):\n",
    "    \"\"\"Takes array of text split by paragraphs and returns array of \n",
    "        arrays of text. Removes elements less than a cutoff length.\"\"\"\n",
    "    my_array = []\n",
    "    for segment in paragraphs:\n",
    "        if len(segment) >= cutoff:\n",
    "            my_array.append([segment])\n",
    "    return my_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_contractions(text): # Do First!\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def split_by_paragraph(text):   # Do Second\n",
    "    \"\"\"Break up string of text into array of paragraphs\"\"\"\n",
    "    return text.split('\\n')\n",
    "\n",
    "def remove_non_ascii(words): # Not needed\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def rejoin_text(paragraphs):\n",
    "    \"\"\"Takes paragraph separated text and rejoins into\n",
    "        single string.\"\"\"\n",
    "    full = \"\"\n",
    "    for segment in paragraphs:\n",
    "        full += segment\n",
    "    return full\n",
    "\n",
    "def remove_stop_words(words):\n",
    "    \"\"\"Takes array of words and removes stop words.\"\"\"\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    return [w for w in words if not w in stop_words] \n",
    "    \n",
    "def find_numbers(words):\n",
    "    \"\"\"Stores all numbers and indices from array of words in a \n",
    "        dictionary mapping indices to numbers and the following word\"\"\"\n",
    "    numbers = dict()\n",
    "    for index, item in enumerate(words):\n",
    "        if item.isdigit():\n",
    "            numbers.update({index: item + \" \" + words[index + 1]})\n",
    "    return numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(filename):\n",
    "    \"\"\"Takes a text file and outputs an array of words and a dictionary of numbers\"\"\"\n",
    "    raw_text = ReadFile(filename)\n",
    "    raw_text = replace_contractions(raw_text)\n",
    "    par = split_by_paragraph(raw_text)\n",
    "    par = remove_spaces(par)\n",
    "    par = strip_until_start(par)\n",
    "    par = strip_after_end(par)\n",
    "    par = to_lowercase(par)\n",
    "    par = remove_punctuation(par)\n",
    "    par = remove_non_ascii(par)\n",
    "    par = rejoin_text(par)\n",
    "    all_words = word_tokenize(par)\n",
    "    cleaned_words = remove_stop_words(all_words)\n",
    "    numbers = find_numbers(cleaned_words)\n",
    "    return cleaned_words, numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now clean all the papers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 1.07633376121521 seconds to run 3 papers.\n"
     ]
    }
   ],
   "source": [
    "# Initialize useful objects\n",
    "cleaned_papers = []\n",
    "all_numbers = []\n",
    "files = [\"sample.txt\", \"sample1.txt\", \"sample2.txt\"]\n",
    "# Start timer!\n",
    "start = time.time()\n",
    "for filename in files:\n",
    "    cleaned_words, numbers = clean_text(filename)\n",
    "    cleaned_papers.append([cleaned_words])\n",
    "    all_numbers.append([numbers])\n",
    "print(f\"It took {time.time() - start} seconds to run {len(files)} papers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
